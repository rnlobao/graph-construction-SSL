{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sets iniciais do projeto.\n",
    "Instalação de bibliotecas necessárias, imports e criação dos datasets de teste\n",
    "| Data set | Classes | Dimension | Points |\n",
    "|----------|---------|-----------|--------|\n",
    "| g241c    | 2       | 241       | 1500   |\n",
    "| g241d    | 2       | 241       | 1500   |\n",
    "| Digit1   | 2       | 241       | 1500   |\n",
    "| USPS     | 2       | 241       | 1500   |\n",
    "| COIL     | 6       | 241       | 1500   |\n",
    "| Text     | 2       | 11,960    | 1500   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação das bibliotecas\n",
    "\n",
    "# %pip install sslbookdata\n",
    "# %pip install scikit-learn\n",
    "# %pip install matplotlib\n",
    "# %pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import sslbookdata\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dos datasets\n",
    "\n",
    "datasets = {\n",
    "    'USPS': sslbookdata.load_usps(0),\n",
    "    'DIGIT': sslbookdata.load_digit1(0),\n",
    "    'GC': sslbookdata.load_g241c(0),\n",
    "    'GN': sslbookdata.load_g241n(0),\n",
    "    # 'COIL': sslbookdata.load_coil2(0),\n",
    "    # 'TEXT': sslbookdata.load_text(0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de PCA, redução da dimensionalidade\n",
    "def analise_de_componente_principal(dataset, num_componentes_principais=50, plotar_imagem=False):\n",
    "    scaler = StandardScaler()\n",
    "    dados_normalizados = scaler.fit_transform(dataset)\n",
    "    pca = PCA(n_components=num_componentes_principais)\n",
    "    dados_reduzidos = pca.fit_transform(dados_normalizados)\n",
    "    variancia_explicada_cumulativa = np.cumsum(pca.explained_variance_ratio_)\n",
    "    if plotar_imagem:\n",
    "        plt.plot(variancia_explicada_cumulativa)\n",
    "        plt.xlabel('Número de Componentes Principais')\n",
    "        plt.ylabel('Variância Explicada Cumulativa')\n",
    "        plt.show()\n",
    "    return dados_reduzidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotar_imagens = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quais serão os passos da construção desse trabalho?\n",
    "1) Geração de um grafo a partir do dataset\n",
    "2) Difusão dos rótulos a partir do grafo\n",
    "3) Resultados obtidos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Geração de um grafo a partir do dataset\n",
    "\n",
    "O conceito abordado é a utilização dos dois primeiros passos do HDBSCAN* a fim de gerar uma árvore geradora mínima, esse é nosso objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeira etapa do HDBSCAN* - Computar core distance para todas instâncias do dataset em relação a um número mínimo de pontos\n",
    "\n",
    "Conceito de Core Distance: Raio mínimo necessário para que uma instância qualquer X seja considerado um objeto core, tendo em vista um número mínimo de pontos próximos (incluindo o próprio ponto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codigo que computa o core distance e retorna um array que contem os raios minimos \n",
    "# e as 3 instâncias que são as mais próximas de determinado ponto\n",
    "\n",
    "def compute_core_distance(datasetToComputeDistance, min_samples):\n",
    "    list_de_raios_minimos = []\n",
    "    list_de_indices_vizinhos = []\n",
    "    neighbors = NearestNeighbors(n_neighbors=min_samples).fit(datasetToComputeDistance)\n",
    "\n",
    "    distances, indices = neighbors.kneighbors(datasetToComputeDistance)\n",
    "    for i in range(len(datasetToComputeDistance)):\n",
    "        list_de_raios_minimos.append(distances[i, -1])\n",
    "        list_de_indices_vizinhos.append(indices[i])\n",
    "    return list_de_raios_minimos, list_de_indices_vizinhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dos raios minimos, das 3 instancias mais proximas de cada ponto \n",
    "# e o dataset em 2d com os 3 pontos mais próximos da primeira instâncias\n",
    "\n",
    "def plotGraphAndStats(dataset_name, datasetForPlot, listOfNeighboringIndices, listOfMinimumRadius):\n",
    "    plt.figure()\n",
    "    for i in range(1, len(datasetForPlot)):\n",
    "        plt.scatter(datasetForPlot[i, 0], datasetForPlot[i, 1], c='b', marker='o', s=8)\n",
    "    for i in listOfNeighboringIndices[0]:\n",
    "        plt.scatter(datasetForPlot[i, 0], datasetForPlot[i, 1], c='r', marker='o', s=8)\n",
    "    plt.scatter(datasetForPlot[0, 0], datasetForPlot[0, 1], c='r', marker='x', s=32, label='Instância 1')\n",
    "    print(f'Raio mínimo para instância 1 ser um core: {listOfMinimumRadius[0]}')\n",
    "    print(f'Lista das intâncias mais perto da instância 1: {listOfNeighboringIndices[0]}')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title(f'Instâncias do dataset {dataset_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_hdbscanstar_step(datasetTreino, numeroMinimoDePontos):\n",
    "    lista_de_raios_minimos, listaDeIndicesVizinhos = compute_core_distance(datasetTreino, numeroMinimoDePontos)\n",
    "    if plotar_imagens:\n",
    "        plotGraphAndStats('USPS', datasetTreino, listaDeIndicesVizinhos, lista_de_raios_minimos)\n",
    "    return lista_de_raios_minimos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segunda etapa do HDBSCAN* - Gerar uma árvore geradora mínima a partir do grafo de alcançabilidade mútua\n",
    "\n",
    "Grafo de alcançabilidade mútua: grafo completo em que os pontos são os objetos do dataset e os pesos das areastas é a distância de alcançabilidade entre os pontos, dado os pontos x1 e x2 essa ditância é calculada pelo maior valor entre: CoreDistance(X1), CoreDistance(X2) e Distância(X1, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMOS QUE CRIAR UMA FUNÇÃO QUE CALCULA A DISTÂNCIA DE ALCANÇABILIDADE ENTRE DOIS PONTOS\n",
    "\n",
    "def mutualReachabilityDistanceCalculation(datasetParaAlcancabilidade, numero_minimo_pontos, printarDistancias=False):\n",
    "    distancia_alcancabilidade_mutua = []\n",
    "    listaDeRaiosMinimos = first_hdbscanstar_step(datasetParaAlcancabilidade, numero_minimo_pontos)\n",
    "    for i in range(len(datasetParaAlcancabilidade)):\n",
    "        alcancabilidade_mutua = []\n",
    "        for j in range(len(datasetParaAlcancabilidade)):\n",
    "            if i != j:\n",
    "                distancia = np.linalg.norm(datasetParaAlcancabilidade[i] - datasetParaAlcancabilidade[j])\n",
    "                reachability_distance = max(distancia, listaDeRaiosMinimos[j])\n",
    "                alcancabilidade_mutua.append(reachability_distance)\n",
    "        distancia_alcancabilidade_mutua.append(alcancabilidade_mutua)\n",
    "\n",
    "    if printarDistancias:\n",
    "        print(\"Distância de Alcançabilidade Mútua:\")\n",
    "        for i in range(len(distancia_alcancabilidade_mutua)):\n",
    "            for j in range(len(distancia_alcancabilidade_mutua[i])):\n",
    "                print(f\"Ponto {i} para Ponto {j}: {distancia_alcancabilidade_mutua[i][j]}\")\n",
    "                \n",
    "    return distancia_alcancabilidade_mutua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação de um grafo de alcançabilidade mutua\n",
    "\n",
    "def createReachabilityGraph(mutual_reachability_distance):\n",
    "    G = nx.Graph()\n",
    "    num_instancias = len(mutual_reachability_distance)\n",
    "    G.add_nodes_from(range(num_instancias))\n",
    "    for i in range(len(mutual_reachability_distance)):\n",
    "        for j in range(len(mutual_reachability_distance[i])):\n",
    "            peso = mutual_reachability_distance[i][j]\n",
    "            G.add_edge(i, j, weight=peso)\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotWeightedGraph(graphForPlot, alreadyWithLabels=False):\n",
    "    if alreadyWithLabels:\n",
    "        node_colors = ['blue' if graphForPlot.nodes[node]['label'] == 1 else 'red' for node in graphForPlot.nodes()]\n",
    "        blue_patch = plt.Line2D([0], [0], marker='o', color='w', label='Label 1', markerfacecolor='blue', markersize=10)\n",
    "        red_patch = plt.Line2D([0], [0], marker='o', color='w', label='Label -1', markerfacecolor='red', markersize=10)\n",
    "        plt.legend(handles=[blue_patch, red_patch])\n",
    "    else:\n",
    "        node_colors = 'lightblue'\n",
    "\n",
    "    pos = nx.spring_layout(graphForPlot)\n",
    "    labels = nx.get_edge_attributes(graphForPlot, 'weight')\n",
    "    edge_labels = {e: f'{labels[e]:.2f}' for e in graphForPlot.edges}\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    nx.draw(graphForPlot, pos, with_labels=False, node_size=40, node_color=node_colors, edge_color='green')\n",
    "    nx.draw_networkx_edges(graphForPlot, pos)\n",
    "    nx.draw_networkx_edge_labels(graphForPlot, pos, edge_labels=edge_labels, font_size=6)\n",
    "    plt.title(\"Árvore Geradora Mínima\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação de uma árvore geradora minima a partir de um grafo\n",
    "\n",
    "def createMinimumSpanningTree(baseGraph, printGraph=False):\n",
    "    minimum_spanning_tree = nx.minimum_spanning_tree(baseGraph)\n",
    "    if printGraph:\n",
    "        plotWeightedGraph(minimum_spanning_tree)\n",
    "    return minimum_spanning_tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_generation(datasetTreino, numero_minimo_de_pontos):\n",
    "    distancia_alcancabilidade_mutua = mutualReachabilityDistanceCalculation(datasetTreino, numero_minimo_de_pontos)\n",
    "    reachability_graph = createReachabilityGraph(distancia_alcancabilidade_mutua)\n",
    "    mst = createMinimumSpanningTree(reachability_graph, plotar_imagens)\n",
    "    return mst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Difusão dos rótulos a partir do grafo\n",
    "\n",
    "Recebendo a MST do primeiro passo vamos usar um método de Gaussian Field Harmonic Function para propagar os \n",
    "rótulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para distribuir os rótulos no grafo\n",
    "\n",
    "def assign_labels_to_graph(graph, labels, number_of_labels=10, assign_id=False, half_nodes=False):\n",
    "    if half_nodes:\n",
    "        positive_nodes = np.where(labels == 1)[0]\n",
    "        negative_nodes = np.where(labels == -1)[0]\n",
    "\n",
    "        random_positive_nodes = np.random.choice(positive_nodes, 5, replace=False)\n",
    "        random_negative_nodes = np.random.choice(negative_nodes, 5, replace=False)\n",
    "        random_nodes = np.concatenate([random_positive_nodes, random_negative_nodes])\n",
    "    else:\n",
    "        random_nodes = np.random.choice(graph.nodes, number_of_labels, replace=False)\n",
    "\n",
    "    nx.set_node_attributes(graph, None, 'label')\n",
    "    for node, label in zip(random_nodes, labels[random_nodes]):\n",
    "        graph.nodes[node]['label'] = label\n",
    "    if assign_id:\n",
    "        nx.set_node_attributes(graph, dict(zip(graph.nodes, range(len(graph.nodes)))), 'ID')\n",
    "    result_dict = {\n",
    "        'random_nodes': random_nodes,\n",
    "        'graph': graph,\n",
    "        'labels': labels[random_nodes]\n",
    "    }\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que reordena as instâncias, colocando as rotuladas na frente\n",
    "\n",
    "def reorder_nodes(graph, random_nodes):\n",
    "    if any(node not in graph.nodes for node in random_nodes):\n",
    "        raise ValueError(\"Um ou mais nós aleatórios não pertencem ao grafo.\")\n",
    "    remaining_nodes = [node for node in graph.nodes if node not in random_nodes]\n",
    "    result_array = np.concatenate((random_nodes, remaining_nodes))\n",
    "    return result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função do Gaussian Field Harmonic Function\n",
    "\n",
    "def gfhf(ordemObjetos, yl, W):\n",
    "  nRotulado = yl.shape[0]\n",
    "  nNaoRotulado = W.shape[0]-nRotulado\n",
    "  nObjetos = W.shape[0]\n",
    "\n",
    "  D = np.zeros(W.shape)\n",
    "  np.fill_diagonal(D, np.sum(W, axis=1))\n",
    "\n",
    "  L= D - W\n",
    "\n",
    "  L = L[ordemObjetos,:]\n",
    "  L = L[:, ordemObjetos]\n",
    "\n",
    "  matRotulado = L[0:nRotulado, 0:nRotulado]\n",
    "  matNaoRotuladoRotulado = L[nRotulado:nObjetos, 0:nRotulado]\n",
    "  matNaoRotulado = L[nRotulado:nObjetos, nRotulado:nObjetos]\n",
    "\n",
    "  yl = np.asmatrix(yl)\n",
    "  matNaoRotuladoRotulado = np.asmatrix(matNaoRotuladoRotulado)\n",
    "  matNaoRotulado = np.asmatrix(matNaoRotulado)\n",
    "\n",
    "  f= -np.linalg.inv(matNaoRotulado)*matNaoRotuladoRotulado*yl\n",
    "  resultado = [0]*nObjetos\n",
    "\n",
    "  for i in range(nRotulado):\n",
    "    resultado[ordemObjetos[i]] = yl[i,0]\n",
    "\n",
    "  ordemNaoRotulado = ordemObjetos[nRotulado:]\n",
    "\n",
    "  for i in range(nNaoRotulado):\n",
    "    resultado[ordemNaoRotulado[i]]= int(1*np.sign(f[i,0]))\n",
    "\n",
    "  return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_labels(mst, label_array, plotar_imagens=False):\n",
    "    resultado_rotulado = assign_labels_to_graph(mst, label_array, number_of_labels=10, assign_id=True)\n",
    "    random_nodes = sorted(resultado_rotulado['random_nodes'])\n",
    "    \n",
    "    ordem_objetos = reorder_nodes(mst, random_nodes)\n",
    "    yl = resultado_rotulado['labels']\n",
    "    # W = nx.linalg.graphmatrix.adjacency_matrix(mst).todense()\n",
    "    W = nx.linalg.graphmatrix.adjacency_matrix(mst)\n",
    "\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            if W[i, j] != 0:\n",
    "                W[i, j] = 1\n",
    "\n",
    "\n",
    "    resultado_GFHF = gfhf(ordem_objetos, yl, W)\n",
    "\n",
    "    # if plotar_imagens:\n",
    "        # print(f'Esses são os objetos rotulados: {random_nodes}')\n",
    "        # np.savetxt('./txt-para-analise/ordem_objetos.txt', ordem_objetos, fmt='%d')\n",
    "        # np.savetxt('./txt-para-analise/label_array.txt', label_array, fmt='%d')\n",
    "        # np.savetxt('./txt-para-analise/yl.txt', yl, fmt='%d')\n",
    "    # np.savetxt('./txt-para-analise/matriz_ajacencia.txt', W, fmt='%d')\n",
    "        # np.savetxt('./txt-para-analise/resultado.txt', resultado_GFHF, fmt='%d')\n",
    "    return resultado_GFHF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Resultados obtidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error rate for dataset USPS: 28.84\n",
      "Average error rate for dataset DIGIT: 55.04\n",
      "Average error rate for dataset GC: 50.00\n",
      "Average error rate for dataset GN: 49.60\n"
     ]
    }
   ],
   "source": [
    "numeroMinimoDePontos = 3\n",
    "reduzir_dimensionalidade = False\n",
    "\n",
    "def calcular_acuracia_media(dataset_treino, label_array, numero_minimo_de_pontos):\n",
    "    acuracias = []\n",
    "    for _ in range(5):\n",
    "        primeiro_resultado = graph_generation(dataset_treino, numero_minimo_de_pontos)\n",
    "        resultado_dos_rotulos = propagate_labels(primeiro_resultado, label_array)\n",
    "\n",
    "        acuracia = accuracy_score(resultado_dos_rotulos, label_array)\n",
    "        acuracias.append(acuracia)\n",
    "    acuracia_media = sum(acuracias) / len(acuracias)\n",
    "    return acuracia_media\n",
    "\n",
    "for dataset_name, dataset_data in datasets.items():\n",
    "    if reduzir_dimensionalidade and dataset_name != \"TEXT\":\n",
    "        dataset_treino = analise_de_componente_principal(dataset_data['data'])\n",
    "    dataset_treino = dataset_data['data']\n",
    "    label_array = dataset_data['target']\n",
    "    acuracia_media = calcular_acuracia_media(dataset_treino, label_array, numeroMinimoDePontos)\n",
    "    error_rate = (1 - acuracia_media) * 100\n",
    "    print(f\"Average error rate for dataset {dataset_name}: {error_rate:.2f}\")\n",
    "\n",
    "# Como resolver Coil, \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
